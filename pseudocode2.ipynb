{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>P/E</th>\n",
       "      <th># Buys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/9/2003</td>\n",
       "      <td>27.905</td>\n",
       "      <td>35.994469</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/10/2003</td>\n",
       "      <td>27.960</td>\n",
       "      <td>36.065413</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/13/2003</td>\n",
       "      <td>28.195</td>\n",
       "      <td>36.368538</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/14/2003</td>\n",
       "      <td>28.485</td>\n",
       "      <td>36.742607</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/15/2003</td>\n",
       "      <td>28.135</td>\n",
       "      <td>36.291144</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>11/25/2019</td>\n",
       "      <td>151.230</td>\n",
       "      <td>28.533935</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>11/26/2019</td>\n",
       "      <td>152.030</td>\n",
       "      <td>28.684879</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>11/27/2019</td>\n",
       "      <td>152.320</td>\n",
       "      <td>28.739596</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>11/29/2019</td>\n",
       "      <td>151.380</td>\n",
       "      <td>28.562237</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>12/2/2019</td>\n",
       "      <td>149.550</td>\n",
       "      <td>28.216955</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4254 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Close        P/E  # Buys\n",
       "0       1/9/2003   27.905  35.994469    14.0\n",
       "1      1/10/2003   27.960  36.065413    13.0\n",
       "2      1/13/2003   28.195  36.368538    13.0\n",
       "3      1/14/2003   28.485  36.742607    13.0\n",
       "4      1/15/2003   28.135  36.291144    13.0\n",
       "...          ...      ...        ...     ...\n",
       "4249  11/25/2019  151.230  28.533935    23.0\n",
       "4250  11/26/2019  152.030  28.684879    23.0\n",
       "4251  11/27/2019  152.320  28.739596    23.0\n",
       "4252  11/29/2019  151.380  28.562237    23.0\n",
       "4253   12/2/2019  149.550  28.216955    23.0\n",
       "\n",
       "[4254 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P/E</th>\n",
       "      <th># Buys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.994469</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.065413</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.368538</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.742607</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.291144</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>28.533935</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>28.684879</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>28.739596</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>28.562237</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>28.216955</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4254 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            P/E  # Buys\n",
       "0     35.994469    14.0\n",
       "1     36.065413    13.0\n",
       "2     36.368538    13.0\n",
       "3     36.742607    13.0\n",
       "4     36.291144    13.0\n",
       "...         ...     ...\n",
       "4249  28.533935    23.0\n",
       "4250  28.684879    23.0\n",
       "4251  28.739596    23.0\n",
       "4252  28.562237    23.0\n",
       "4253  28.216955    23.0\n",
       "\n",
       "[4254 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ~df.columns.isin([\"Close\", \"Date\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        27.905\n",
       "1        27.960\n",
       "2        28.195\n",
       "3        28.485\n",
       "4        28.135\n",
       "         ...   \n",
       "4249    151.230\n",
       "4250    152.030\n",
       "4251    152.320\n",
       "4252    151.380\n",
       "4253    149.550\n",
       "Name: Close, Length: 4254, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.loc[:,\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       27.905\n",
       "1       27.960\n",
       "2       28.195\n",
       "3       28.485\n",
       "4       28.135\n",
       "         ...  \n",
       "2547    27.760\n",
       "2548    27.370\n",
       "2549    27.370\n",
       "2550    27.810\n",
       "2551    27.800\n",
       "Name: Close, Length: 2552, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small\n",
    "df = df_small\n",
    "\n",
    "target = \"Close\"\n",
    "\n",
    "feature_df = df.loc[:, ~df.columns.isin([target, \"Date\"])]\n",
    "target_df = df.loc[:, target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, target_df, test_size=0.4, shuffle=False) \n",
    "\n",
    "y_train.iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\polet\\OneDrive\\University College London\\Turintech\\Code\\Sun 2014\\pseudocode2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/polet/OneDrive/University%20College%20London/Turintech/Code/Sun%202014/pseudocode2.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39mlist\u001b[39;49m(df\u001b[39m.\u001b[39;49mcolumns)[\u001b[39mFalse\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m]\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "list(df.columns)[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/10/2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/13/2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/14/2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/15/2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/16/2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>11/25/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>11/26/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>11/27/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>11/29/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>12/2/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4253 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date\n",
       "0      1/10/2003\n",
       "1      1/13/2003\n",
       "2      1/14/2003\n",
       "3      1/15/2003\n",
       "4      1/16/2003\n",
       "...          ...\n",
       "4248  11/25/2019\n",
       "4249  11/26/2019\n",
       "4250  11/27/2019\n",
       "4251  11/29/2019\n",
       "4252   12/2/2019\n",
       "\n",
       "[4253 rows x 1 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.iloc[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aapl = pd.read_csv(\"df_aaple.csv\")\n",
    "df_small = df_aapl.iloc[:,:4]\n",
    "df_small.drop(columns=\"Adj. Close\", inplace=True)\n",
    "df_small[\"P/E\"] = df_small[\"P/E (LTM)\"]\n",
    "df_small.drop(columns=\"P/E (LTM)\", inplace=True)\n",
    "df_small[\"# Buys\"] = df_aapl[\"# Buys\"]\n",
    "\n",
    "df_small_raw = df_small\n",
    "\n",
    "def algo(df, target, max_lag):\n",
    "\n",
    "    # Step 1: Tranformation for stationarity d\n",
    "    # Here features are everything except for the date\n",
    "    features = [n for n in list(df.columns) if n != \"Date\"]\n",
    "\n",
    "    for feature in features:\n",
    "        result = adfuller(df[feature], autolag=None)\n",
    "        counter = 0\n",
    "        while result[1] > 0.05:\n",
    "            df[feature] = df[feature] - df[feature].shift(1)\n",
    "            #df_small.dropna()\n",
    "            counter += 1\n",
    "            #dropna(inplace=False) because it drops one observation for each feature\n",
    "            result = adfuller(df.dropna()[feature], autolag=None)\n",
    "        print(f'Order of integration for feature \"{feature}\" is {counter}')\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    feature_df = df.loc[:, ~df.columns.isin([target, \"Date\"])]\n",
    "    target_df = df.loc[:, target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_df, target_df, test_size=0.4, shuffle=False) \n",
    "\n",
    "    # Step 2: Building a univariate model and finding the optimal l\n",
    "    BICs = []\n",
    "    for i in list(range(max_lag)):\n",
    "        model = AutoReg(y_train, lags=i).fit()\n",
    "        BICs.append(model.bic)\n",
    "\n",
    "    min_bic_ind = BICs.index(min(BICs))\n",
    "\n",
    "    # model = AutoReg(df_small.iloc[:,1], lags=min_bic_ind).fit()\n",
    "    # model.summary()\n",
    "\n",
    "\n",
    "    # Step 2: Bulding augmented model and finding the optimal w for each Xi\n",
    "    \n",
    "    Xs = list(X_train.columns)\n",
    "\n",
    "    # Defining dictionary to store all augmented models\n",
    "    aug_models = {}\n",
    "    feature_n_dfs = {}\n",
    "    feature_n_dfs_merge = []\n",
    "    \n",
    "    for n in list(range(1, len(features))):\n",
    "        columns = []\n",
    "        for i in list(range(1, max_lag+1)):\n",
    "            columns.append(features[n]+\".L\"+str(i))\n",
    "\n",
    "        feature_n_df = pd.DataFrame(columns=columns)\n",
    "        for i in list(range(max_lag)):\n",
    "            feature_n_df[columns[i]] = df[features[n]].shift(i+1)\n",
    "\n",
    "        feature_n_df.fillna(1, inplace=True)\n",
    "\n",
    "        BICs = []\n",
    "        #Why do I have max_lag-1 and then i+1?\n",
    "        for i in list(range(max_lag-1)):\n",
    "            model = AutoReg(df.iloc[:,1], lags=min_bic_ind, exog=feature_n_df.iloc[:,:i+1]).fit()\n",
    "            BICs.append(model.bic)\n",
    "\n",
    "        min_bic_ind_aug = BICs.index(min(BICs))\n",
    "        #Full and Partial autocorrelation plot?\n",
    "        feature_n_df1 = feature_n_df\n",
    "        feature_n_df = feature_n_df.iloc[:,:min_bic_ind_aug+1]\n",
    "\n",
    "        model = AutoReg(df.iloc[:,1], lags=min_bic_ind, exog=feature_n_df).fit()\n",
    "\n",
    "        if grangercausalitytests(df[[features[1], features[0]]], maxlag=[min_bic_ind_aug+1])[min_bic_ind_aug+1][0]['params_ftest'][1] <= 0.05:\n",
    "            aug_models[features[n]] = model\n",
    "            feature_n_dfs[features[n]] = feature_n_df1\n",
    "            feature_n_dfs_merge.append(feature_n_df)\n",
    "            #model.summary()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "        # aug_models[features[n]] = model\n",
    "        # feature_n_dfs[features[n]] = feature_n_df1\n",
    "        # feature_n_dfs_merge.append(feature_n_df)\n",
    "        # #model.summary()\n",
    "    feature_n_dfs_merge = pd.concat(feature_n_dfs_merge, axis=1)\n",
    "\n",
    "    fin_model = AutoReg(df.iloc[:,1], lags=min_bic_ind, exog=feature_n_dfs_merge).fit()\n",
    "\n",
    "    MAE = np.nanmean(abs(fin_model.predict() - df.iloc[:,1]))\n",
    "\n",
    "    return fin_model, aug_models, feature_n_dfs, feature_n_dfs_merge, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aapl = pd.read_csv(\"df_aaple.csv\")\n",
    "df_small = df_aapl.iloc[:,:4]\n",
    "df_small.drop(columns=\"Adj. Close\", inplace=True)\n",
    "df_small[\"P/E\"] = df_small[\"P/E (LTM)\"]\n",
    "df_small.drop(columns=\"P/E (LTM)\", inplace=True)\n",
    "df_small[\"# Buys\"] = df_aapl[\"# Buys\"]\n",
    "\n",
    "df_small_raw = df_small\n",
    "\n",
    "def algo(df, target, max_lag):\n",
    "\n",
    "    # Step 1: Tranformation for stationarity d\n",
    "    # Here features are everything except for the date\n",
    "    features = [n for n in list(df.columns) if n != \"Date\"]\n",
    "\n",
    "    for feature in features:\n",
    "        result = adfuller(df[feature], autolag=None)\n",
    "        counter = 0\n",
    "        while result[1] > 0.05:\n",
    "            df[feature] = df[feature] - df[feature].shift(1)\n",
    "            #df_small.dropna()\n",
    "            counter += 1\n",
    "            #dropna(inplace=False) because it drops one observation for each feature\n",
    "            result = adfuller(df.dropna()[feature], autolag=None)\n",
    "        print(f'Order of integration for feature \"{feature}\" is {counter}')\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    feature_df = df.loc[:, ~df.columns.isin([target, \"Date\"])]\n",
    "    target_df = df.loc[:, target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_df, target_df, test_size=0.4, shuffle=False) \n",
    "\n",
    "    # Step 2: Building a univariate model and finding the optimal l\n",
    "    BICs = []\n",
    "    for i in list(range(max_lag)):\n",
    "        model = AutoReg(y_train, lags=i).fit()\n",
    "        BICs.append(model.bic)\n",
    "\n",
    "    min_bic_ind = BICs.index(min(BICs))\n",
    "\n",
    "    # model = AutoReg(df_small.iloc[:,1], lags=min_bic_ind).fit()\n",
    "    # model.summary()\n",
    "\n",
    "\n",
    "    # Step 2: Bulding augmented model and finding the optimal w for each Xi\n",
    "    \n",
    "    Xs = list(X_train.columns)\n",
    "\n",
    "    # Defining dictionary to store all augmented models\n",
    "    aug_models = {}\n",
    "    feature_n_dfs = {}\n",
    "    feature_n_dfs_merge = []\n",
    "    \n",
    "    for n in list(range(len(Xs))):\n",
    "        columns = []\n",
    "        for i in list(range(1, max_lag+1)):\n",
    "            columns.append(Xs[n]+\".L\"+str(i))\n",
    "\n",
    "        feature_n_df = pd.DataFrame(columns=columns)\n",
    "        for i in list(range(max_lag)):\n",
    "            feature_n_df[columns[i]] = X_train[Xs[n]].shift(i+1)\n",
    "\n",
    "        feature_n_df.fillna(1, inplace=True)\n",
    "\n",
    "        BICs = []\n",
    "        #Why do I have max_lag-1 and then i+1?\n",
    "        for i in list(range(max_lag-1)):\n",
    "            model = AutoReg(y_train, lags=min_bic_ind, exog=feature_n_df.iloc[:,:i+1]).fit()\n",
    "            BICs.append(model.bic)\n",
    "\n",
    "        min_bic_ind_aug = BICs.index(min(BICs))\n",
    "        #Full and Partial autocorrelation plot?\n",
    "        feature_n_df1 = feature_n_df\n",
    "        feature_n_df = feature_n_df.iloc[:,:min_bic_ind_aug+1]\n",
    "\n",
    "        model = AutoReg(y_train, lags=min_bic_ind, exog=feature_n_df).fit()\n",
    "\n",
    "        gr_test_df = pd.concat([X_train[Xs[n]], y_train], axis=1)\n",
    "\n",
    "        if grangercausalitytests(gr_test_df, maxlag=[min_bic_ind_aug+1])[min_bic_ind_aug+1][0]['params_ftest'][1] <= 0.05:\n",
    "            aug_models[Xs[n]] = model\n",
    "            feature_n_dfs[Xs[n]] = feature_n_df1\n",
    "            feature_n_dfs_merge.append(feature_n_df)\n",
    "            #model.summary()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "        # aug_models[features[n]] = model\n",
    "        # feature_n_dfs[features[n]] = feature_n_df1\n",
    "        # feature_n_dfs_merge.append(feature_n_df)\n",
    "        # #model.summary()\n",
    "    feature_n_dfs_merge = pd.concat(feature_n_dfs_merge, axis=1)\n",
    "\n",
    "    fin_model = AutoReg(y_train, lags=min_bic_ind, exog=feature_n_dfs_merge).fit()\n",
    "\n",
    "    MAE = np.nanmean(abs(fin_model.predict() - df.iloc[:,1]))\n",
    "\n",
    "    return fin_model, aug_models, feature_n_dfs, feature_n_dfs_merge, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order of integration for feature \"Close\" is 1\n",
      "Order of integration for feature \"P/E\" is 1\n",
      "Order of integration for feature \"# Buys\" is 1\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=8.5343  , p=0.0035  , df_denom=2547, df_num=1\n",
      "ssr based chi2 test:   chi2=8.5443  , p=0.0035  , df=1\n",
      "likelihood ratio test: chi2=8.5300  , p=0.0035  , df=1\n",
      "parameter F test:         F=8.5343  , p=0.0035  , df_denom=2547, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=1.1563  , p=0.2823  , df_denom=2547, df_num=1\n",
      "ssr based chi2 test:   chi2=1.1576  , p=0.2820  , df=1\n",
      "likelihood ratio test: chi2=1.1574  , p=0.2820  , df=1\n",
      "parameter F test:         F=1.1563  , p=0.2823  , df_denom=2547, df_num=1\n"
     ]
    }
   ],
   "source": [
    "fin_model, aug_models, dfs, dfs_merged, MAE = algo(df=df_small, target=\"Close\", max_lag=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>AutoReg Model Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Close</td>      <th>  No. Observations:  </th>   <td>4253</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>           <td>AutoReg-X(6)</td>   <th>  Log Likelihood     </th> <td>-4793.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>         <td>Conditional MLE</td> <th>  S.D. of innovations</th>   <td>0.748</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sat, 28 May 2022</td> <th>  AIC                </th> <td>9609.743</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>19:12:33</td>     <th>  BIC                </th> <td>9679.636</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sample:</th>                <td>6</td>        <th>  HQIC               </th> <td>9634.443</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                     <td>4253</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    0.0357</td> <td>    0.012</td> <td>    3.085</td> <td> 0.002</td> <td>    0.013</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L1</th> <td>   -0.0801</td> <td>    0.017</td> <td>   -4.665</td> <td> 0.000</td> <td>   -0.114</td> <td>   -0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L2</th> <td>   -0.0819</td> <td>    0.017</td> <td>   -4.763</td> <td> 0.000</td> <td>   -0.116</td> <td>   -0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L3</th> <td>    0.0756</td> <td>    0.017</td> <td>    4.392</td> <td> 0.000</td> <td>    0.042</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L4</th> <td>   -0.0505</td> <td>    0.015</td> <td>   -3.287</td> <td> 0.001</td> <td>   -0.081</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L5</th> <td>   -0.0713</td> <td>    0.015</td> <td>   -4.652</td> <td> 0.000</td> <td>   -0.101</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L6</th> <td>   -0.0256</td> <td>    0.015</td> <td>   -1.673</td> <td> 0.094</td> <td>   -0.056</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P/E.L1</th>   <td>   -0.0444</td> <td>    0.016</td> <td>   -2.731</td> <td> 0.006</td> <td>   -0.076</td> <td>   -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P/E.L2</th>   <td>   -0.0053</td> <td>    0.016</td> <td>   -0.324</td> <td> 0.746</td> <td>   -0.037</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P/E.L3</th>   <td>   -0.0867</td> <td>    0.016</td> <td>   -5.331</td> <td> 0.000</td> <td>   -0.119</td> <td>   -0.055</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Roots</caption>\n",
       "<tr>\n",
       "    <td></td>   <th>            Real</th>  <th>         Imaginary</th> <th>         Modulus</th>  <th>        Frequency</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.1</th> <td>           1.2810</td> <td>          -0.9112j</td> <td>           1.5720</td> <td>          -0.0984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.2</th> <td>           1.2810</td> <td>          +0.9112j</td> <td>           1.5720</td> <td>           0.0984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.3</th> <td>          -0.5319</td> <td>          -1.5310j</td> <td>           1.6207</td> <td>          -0.3032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.4</th> <td>          -0.5319</td> <td>          +1.5310j</td> <td>           1.6207</td> <td>           0.3032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.5</th> <td>          -2.1429</td> <td>          -1.1952j</td> <td>           2.4537</td> <td>          -0.4190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.6</th> <td>          -2.1429</td> <td>          +1.1952j</td> <td>           2.4537</td> <td>           0.4190</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            AutoReg Model Results                             \n",
       "==============================================================================\n",
       "Dep. Variable:                  Close   No. Observations:                 4253\n",
       "Model:                   AutoReg-X(6)   Log Likelihood               -4793.871\n",
       "Method:               Conditional MLE   S.D. of innovations              0.748\n",
       "Date:                Sat, 28 May 2022   AIC                           9609.743\n",
       "Time:                        19:12:33   BIC                           9679.636\n",
       "Sample:                             6   HQIC                          9634.443\n",
       "                                 4253                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0357      0.012      3.085      0.002       0.013       0.058\n",
       "Close.L1      -0.0801      0.017     -4.665      0.000      -0.114      -0.046\n",
       "Close.L2      -0.0819      0.017     -4.763      0.000      -0.116      -0.048\n",
       "Close.L3       0.0756      0.017      4.392      0.000       0.042       0.109\n",
       "Close.L4      -0.0505      0.015     -3.287      0.001      -0.081      -0.020\n",
       "Close.L5      -0.0713      0.015     -4.652      0.000      -0.101      -0.041\n",
       "Close.L6      -0.0256      0.015     -1.673      0.094      -0.056       0.004\n",
       "P/E.L1        -0.0444      0.016     -2.731      0.006      -0.076      -0.013\n",
       "P/E.L2        -0.0053      0.016     -0.324      0.746      -0.037       0.027\n",
       "P/E.L3        -0.0867      0.016     -5.331      0.000      -0.119      -0.055\n",
       "                                    Roots                                    \n",
       "=============================================================================\n",
       "                  Real          Imaginary           Modulus         Frequency\n",
       "-----------------------------------------------------------------------------\n",
       "AR.1            1.2810           -0.9112j            1.5720           -0.0984\n",
       "AR.2            1.2810           +0.9112j            1.5720            0.0984\n",
       "AR.3           -0.5319           -1.5310j            1.6207           -0.3032\n",
       "AR.4           -0.5319           +1.5310j            1.6207            0.3032\n",
       "AR.5           -2.1429           -1.1952j            2.4537           -0.4190\n",
       "AR.6           -2.1429           +1.1952j            2.4537            0.4190\n",
       "-----------------------------------------------------------------------------\n",
       "\"\"\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4564910413023767"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            NaN\n",
       "1            NaN\n",
       "2            NaN\n",
       "3            NaN\n",
       "4            NaN\n",
       "          ...   \n",
       "4248   -0.098360\n",
       "4249   -0.133332\n",
       "4250   -0.263861\n",
       "4251    0.030596\n",
       "4252    0.030820\n",
       "Length: 4253, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = fin_model.predict()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P/E.L1</th>\n",
       "      <th>P/E.L2</th>\n",
       "      <th>P/E.L3</th>\n",
       "      <th>P/E.L4</th>\n",
       "      <th>P/E.L5</th>\n",
       "      <th>P/E.L6</th>\n",
       "      <th>P/E.L7</th>\n",
       "      <th>P/E.L8</th>\n",
       "      <th>P/E.L9</th>\n",
       "      <th>P/E.L10</th>\n",
       "      <th>...</th>\n",
       "      <th>P/E.L21</th>\n",
       "      <th>P/E.L22</th>\n",
       "      <th>P/E.L23</th>\n",
       "      <th>P/E.L24</th>\n",
       "      <th>P/E.L25</th>\n",
       "      <th>P/E.L26</th>\n",
       "      <th>P/E.L27</th>\n",
       "      <th>P/E.L28</th>\n",
       "      <th>P/E.L29</th>\n",
       "      <th>P/E.L30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.303125</td>\n",
       "      <td>0.070944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.374069</td>\n",
       "      <td>0.303125</td>\n",
       "      <td>0.070944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.451463</td>\n",
       "      <td>0.374069</td>\n",
       "      <td>0.303125</td>\n",
       "      <td>0.070944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>0.020755</td>\n",
       "      <td>-0.026415</td>\n",
       "      <td>-0.145283</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.069811</td>\n",
       "      <td>0.360377</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.045283</td>\n",
       "      <td>0.181132</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149056</td>\n",
       "      <td>0.509433</td>\n",
       "      <td>-1.056278</td>\n",
       "      <td>-0.407115</td>\n",
       "      <td>0.201581</td>\n",
       "      <td>-0.450593</td>\n",
       "      <td>-0.142292</td>\n",
       "      <td>-0.230237</td>\n",
       "      <td>0.400198</td>\n",
       "      <td>-0.025692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>0.309434</td>\n",
       "      <td>0.020755</td>\n",
       "      <td>-0.026415</td>\n",
       "      <td>-0.145283</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.069811</td>\n",
       "      <td>0.360377</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.045283</td>\n",
       "      <td>0.181132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652830</td>\n",
       "      <td>0.149056</td>\n",
       "      <td>0.509433</td>\n",
       "      <td>-1.056278</td>\n",
       "      <td>-0.407115</td>\n",
       "      <td>0.201581</td>\n",
       "      <td>-0.450593</td>\n",
       "      <td>-0.142292</td>\n",
       "      <td>-0.230237</td>\n",
       "      <td>0.400198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.309434</td>\n",
       "      <td>0.020755</td>\n",
       "      <td>-0.026415</td>\n",
       "      <td>-0.145283</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.069811</td>\n",
       "      <td>0.360377</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.045283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256604</td>\n",
       "      <td>0.652830</td>\n",
       "      <td>0.149056</td>\n",
       "      <td>0.509433</td>\n",
       "      <td>-1.056278</td>\n",
       "      <td>-0.407115</td>\n",
       "      <td>0.201581</td>\n",
       "      <td>-0.450593</td>\n",
       "      <td>-0.142292</td>\n",
       "      <td>-0.230237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>0.054717</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.309434</td>\n",
       "      <td>0.020755</td>\n",
       "      <td>-0.026415</td>\n",
       "      <td>-0.145283</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.069811</td>\n",
       "      <td>0.360377</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335849</td>\n",
       "      <td>-0.256604</td>\n",
       "      <td>0.652830</td>\n",
       "      <td>0.149056</td>\n",
       "      <td>0.509433</td>\n",
       "      <td>-1.056278</td>\n",
       "      <td>-0.407115</td>\n",
       "      <td>0.201581</td>\n",
       "      <td>-0.450593</td>\n",
       "      <td>-0.142292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>-0.177358</td>\n",
       "      <td>0.054717</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.309434</td>\n",
       "      <td>0.020755</td>\n",
       "      <td>-0.026415</td>\n",
       "      <td>-0.145283</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.069811</td>\n",
       "      <td>0.360377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233962</td>\n",
       "      <td>0.335849</td>\n",
       "      <td>-0.256604</td>\n",
       "      <td>0.652830</td>\n",
       "      <td>0.149056</td>\n",
       "      <td>0.509433</td>\n",
       "      <td>-1.056278</td>\n",
       "      <td>-0.407115</td>\n",
       "      <td>0.201581</td>\n",
       "      <td>-0.450593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4253 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        P/E.L1    P/E.L2    P/E.L3    P/E.L4    P/E.L5    P/E.L6    P/E.L7  \\\n",
       "0     1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "1     0.070944  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "2     0.303125  0.070944  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "3     0.374069  0.303125  0.070944  1.000000  1.000000  1.000000  1.000000   \n",
       "4    -0.451463  0.374069  0.303125  0.070944  1.000000  1.000000  1.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4248  0.020755 -0.026415 -0.145283  0.009434  0.069811  0.360377  0.141509   \n",
       "4249  0.309434  0.020755 -0.026415 -0.145283  0.009434  0.069811  0.360377   \n",
       "4250  0.150943  0.309434  0.020755 -0.026415 -0.145283  0.009434  0.069811   \n",
       "4251  0.054717  0.150943  0.309434  0.020755 -0.026415 -0.145283  0.009434   \n",
       "4252 -0.177358  0.054717  0.150943  0.309434  0.020755 -0.026415 -0.145283   \n",
       "\n",
       "        P/E.L8    P/E.L9   P/E.L10  ...   P/E.L21   P/E.L22   P/E.L23  \\\n",
       "0     1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
       "1     1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
       "2     1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
       "3     1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
       "4     1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4248  0.045283  0.181132  0.028302  ...  0.149056  0.509433 -1.056278   \n",
       "4249  0.141509  0.045283  0.181132  ...  0.652830  0.149056  0.509433   \n",
       "4250  0.360377  0.141509  0.045283  ... -0.256604  0.652830  0.149056   \n",
       "4251  0.069811  0.360377  0.141509  ...  0.335849 -0.256604  0.652830   \n",
       "4252  0.009434  0.069811  0.360377  ... -0.233962  0.335849 -0.256604   \n",
       "\n",
       "       P/E.L24   P/E.L25   P/E.L26   P/E.L27   P/E.L28   P/E.L29   P/E.L30  \n",
       "0     1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "1     1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "2     1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "3     1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "4     1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4248 -0.407115  0.201581 -0.450593 -0.142292 -0.230237  0.400198 -0.025692  \n",
       "4249 -1.056278 -0.407115  0.201581 -0.450593 -0.142292 -0.230237  0.400198  \n",
       "4250  0.509433 -1.056278 -0.407115  0.201581 -0.450593 -0.142292 -0.230237  \n",
       "4251  0.149056  0.509433 -1.056278 -0.407115  0.201581 -0.450593 -0.142292  \n",
       "4252  0.652830  0.149056  0.509433 -1.056278 -0.407115  0.201581 -0.450593  \n",
       "\n",
       "[4253 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['P/E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>AutoReg Model Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Close</td>      <th>  No. Observations:  </th>   <td>4253</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>           <td>AutoReg-X(8)</td>   <th>  Log Likelihood     </th> <td>-4293.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>         <td>Conditional MLE</td> <th>  S.D. of innovations</th>   <td>0.665</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 13 May 2022</td> <th>  AIC                </th> <td>8614.314</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>14:47:41</td>     <th>  BIC                </th> <td>8703.263</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sample:</th>                <td>8</td>        <th>  HQIC               </th> <td>8645.749</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                     <td>4253</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    0.0376</td> <td>    0.010</td> <td>    3.649</td> <td> 0.000</td> <td>    0.017</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L1</th> <td>   -0.0688</td> <td>    0.015</td> <td>   -4.498</td> <td> 0.000</td> <td>   -0.099</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L2</th> <td>   -0.0807</td> <td>    0.015</td> <td>   -5.268</td> <td> 0.000</td> <td>   -0.111</td> <td>   -0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L3</th> <td>    0.0524</td> <td>    0.015</td> <td>    3.410</td> <td> 0.001</td> <td>    0.022</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L4</th> <td>   -0.0549</td> <td>    0.014</td> <td>   -4.015</td> <td> 0.000</td> <td>   -0.082</td> <td>   -0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L5</th> <td>   -0.0479</td> <td>    0.014</td> <td>   -3.498</td> <td> 0.000</td> <td>   -0.075</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L6</th> <td>   -0.0278</td> <td>    0.014</td> <td>   -2.025</td> <td> 0.043</td> <td>   -0.055</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L7</th> <td>    0.0164</td> <td>    0.014</td> <td>    1.197</td> <td> 0.231</td> <td>   -0.010</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Close.L8</th> <td>   -0.0605</td> <td>    0.014</td> <td>   -4.441</td> <td> 0.000</td> <td>   -0.087</td> <td>   -0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P/E.L1</th>   <td>    0.4255</td> <td>    0.013</td> <td>   32.928</td> <td> 0.000</td> <td>    0.400</td> <td>    0.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P/E.L2</th>   <td>   -0.0355</td> <td>    0.014</td> <td>   -2.454</td> <td> 0.014</td> <td>   -0.064</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P/E.L3</th>   <td>    0.0026</td> <td>    0.014</td> <td>    0.178</td> <td> 0.858</td> <td>   -0.026</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P/E.L4</th>   <td>   -0.0700</td> <td>    0.014</td> <td>   -4.833</td> <td> 0.000</td> <td>   -0.098</td> <td>   -0.042</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Roots</caption>\n",
       "<tr>\n",
       "    <td></td>   <th>            Real</th>  <th>         Imaginary</th> <th>         Modulus</th>  <th>        Frequency</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.1</th> <td>          -1.2627</td> <td>          -0.5953j</td> <td>           1.3960</td> <td>          -0.4299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.2</th> <td>          -1.2627</td> <td>          +0.5953j</td> <td>           1.3960</td> <td>           0.4299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.3</th> <td>          -0.4911</td> <td>          -1.2416j</td> <td>           1.3352</td> <td>          -0.3100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.4</th> <td>          -0.4911</td> <td>          +1.2416j</td> <td>           1.3352</td> <td>           0.3100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.5</th> <td>           1.2768</td> <td>          -0.6376j</td> <td>           1.4272</td> <td>          -0.0737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.6</th> <td>           1.2768</td> <td>          +0.6376j</td> <td>           1.4272</td> <td>           0.0737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.7</th> <td>           0.6122</td> <td>          -1.4002j</td> <td>           1.5282</td> <td>          -0.1844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AR.8</th> <td>           0.6122</td> <td>          +1.4002j</td> <td>           1.5282</td> <td>           0.1844</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            AutoReg Model Results                             \n",
       "==============================================================================\n",
       "Dep. Variable:                  Close   No. Observations:                 4253\n",
       "Model:                   AutoReg-X(8)   Log Likelihood               -4293.157\n",
       "Method:               Conditional MLE   S.D. of innovations              0.665\n",
       "Date:                Fri, 13 May 2022   AIC                           8614.314\n",
       "Time:                        14:47:41   BIC                           8703.263\n",
       "Sample:                             8   HQIC                          8645.749\n",
       "                                 4253                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0376      0.010      3.649      0.000       0.017       0.058\n",
       "Close.L1      -0.0688      0.015     -4.498      0.000      -0.099      -0.039\n",
       "Close.L2      -0.0807      0.015     -5.268      0.000      -0.111      -0.051\n",
       "Close.L3       0.0524      0.015      3.410      0.001       0.022       0.082\n",
       "Close.L4      -0.0549      0.014     -4.015      0.000      -0.082      -0.028\n",
       "Close.L5      -0.0479      0.014     -3.498      0.000      -0.075      -0.021\n",
       "Close.L6      -0.0278      0.014     -2.025      0.043      -0.055      -0.001\n",
       "Close.L7       0.0164      0.014      1.197      0.231      -0.010       0.043\n",
       "Close.L8      -0.0605      0.014     -4.441      0.000      -0.087      -0.034\n",
       "P/E.L1         0.4255      0.013     32.928      0.000       0.400       0.451\n",
       "P/E.L2        -0.0355      0.014     -2.454      0.014      -0.064      -0.007\n",
       "P/E.L3         0.0026      0.014      0.178      0.858      -0.026       0.031\n",
       "P/E.L4        -0.0700      0.014     -4.833      0.000      -0.098      -0.042\n",
       "                                    Roots                                    \n",
       "=============================================================================\n",
       "                  Real          Imaginary           Modulus         Frequency\n",
       "-----------------------------------------------------------------------------\n",
       "AR.1           -1.2627           -0.5953j            1.3960           -0.4299\n",
       "AR.2           -1.2627           +0.5953j            1.3960            0.4299\n",
       "AR.3           -0.4911           -1.2416j            1.3352           -0.3100\n",
       "AR.4           -0.4911           +1.2416j            1.3352            0.3100\n",
       "AR.5            1.2768           -0.6376j            1.4272           -0.0737\n",
       "AR.6            1.2768           +0.6376j            1.4272            0.0737\n",
       "AR.7            0.6122           -1.4002j            1.5282           -0.1844\n",
       "AR.8            0.6122           +1.4002j            1.5282            0.1844\n",
       "-----------------------------------------------------------------------------\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_models['P/E'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cgi import test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from arch.unitroot.cointegration import engle_granger\n",
    "\n",
    "\n",
    "from FRUFS import FRUFS\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import joblib, gc\n",
    "# import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from FRUFS import FRUFS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import logging\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from Sun_Model_Class import Sun_Model\n",
    "# logging.INFO\n",
    "\n",
    "import pmdarima as pmd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def algo(df, target, max_lag, stationarity_method, test_size):\n",
    "\n",
    "    # Cleaning column names\n",
    "    # col_names = list(df.columns)\n",
    "    # new_names = []\n",
    "    # for n in col_names:\n",
    "    #     n = re.sub('[^A-Za-z0-9#% ]+', '', n)\n",
    "    #     n = re.sub('[^A-Za-z0-9% ]+', 'n', n)\n",
    "    #     n = re.sub('[^A-Za-z0-9 ]+', 'pc', n)\n",
    "    #     n = re.sub('[^A-Za-z0-9]+', '_', n)\n",
    "    #     new_names.append(n)\n",
    "    # df.columns = new_names\n",
    "\n",
    "    # Step 1: Tranformation for stationarity d\n",
    "    # Here features are everything except for the date\n",
    "    feature_df = df.loc[:, ~df.columns.isin([target, \"Date\"])]\n",
    "    target_df = df.loc[:, target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_df, target_df, test_size=test_size, shuffle=False)\n",
    "    \n",
    "    # Resetting index for later modelling purposes\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Dataframe to perform ADF test\n",
    "    staionarity_df = pd.concat([y_train, X_train], axis=1)\n",
    "\n",
    "    features = list(staionarity_df.columns)\n",
    "\n",
    "    # features = [n for n in list(X_train.columns) if n != \"Date\"]\n",
    "    \n",
    "    # Coppying dataframes for stationarity and back-transformation purposes\n",
    "    yx_train_original = staionarity_df.copy()\n",
    "    y_original = y_train.copy()\n",
    "    y_logged = np.log(y_original)\n",
    "\n",
    "    orders_of_integ = {}\n",
    "    const_counters = {}\n",
    "\n",
    "    for feature in features:\n",
    "        result = adfuller(staionarity_df[feature], autolag=\"t-stat\")\n",
    "        counter = 0\n",
    "        if stationarity_method == 0:\n",
    "            while result[1] >= 0.01:\n",
    "                staionarity_df[feature] = staionarity_df[feature] - staionarity_df[feature].shift(1)\n",
    "                #df_small.dropna()\n",
    "                counter += 1\n",
    "                #dropna(inplace=False) because it drops one observation for each feature\n",
    "                result = adfuller(staionarity_df.dropna()[feature], autolag=\"t-stat\")\n",
    "            print(f'Order of integration for feature \"{feature}\" is {counter}')\n",
    "            orders_of_integ[feature] = counter\n",
    "        elif stationarity_method == 1:\n",
    "            feature_logged = np.log(staionarity_df[feature])\n",
    "            inf_count = np.isinf(feature_logged).sum()\n",
    "            const_counter = 0\n",
    "            # If inf count is greater than 0 it is likely that original series contains 0s or negative values\n",
    "            # Hence we add a constant to the series and only then apply log tranformations until there are no zeroes/nrgative values\n",
    "            while inf_count > 0:\n",
    "                if const_counter == 0:\n",
    "                    feature_with_constant = staionarity_df[feature] + 1\n",
    "                    feature_logged = np.log(feature_with_constant)\n",
    "                    inf_count = np.isinf(feature_logged).sum()\n",
    "                    const_counter += 1\n",
    "                else:\n",
    "                    feature_with_constant = feature_with_constant + 1\n",
    "                    feature_logged = np.log(feature_with_constant)\n",
    "                    inf_count = np.isinf(feature_logged).sum()\n",
    "                    const_counter += 1\n",
    "\n",
    "            while result[1] >= 0.01:\n",
    "                feature_differenced = feature_logged.diff()\n",
    "                staionarity_df[feature] = feature_differenced\n",
    "                #df_small.dropna()\n",
    "                counter += 1\n",
    "                #dropna(inplace=False) because it drops one observation for each feature\n",
    "                result = adfuller(staionarity_df.dropna()[feature], autolag=\"t-stat\")\n",
    "            print(f'Order of integration for feature \"{feature}\" is {counter}')\n",
    "            orders_of_integ[feature] = counter\n",
    "            const_counters[feature] = const_counter\n",
    "\n",
    "    staionarity_df.dropna(inplace=True)\n",
    "    staionarity_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    y_train = staionarity_df[target]\n",
    "    X_train = staionarity_df[[n for n in list(staionarity_df.columns) if n != target]]\n",
    "\n",
    "    #Error Correction Model\n",
    "    cointegr_dict = {}\n",
    "    ECM_residuals = {}\n",
    "    ECM_OLSs = {}\n",
    "    x_features = list(X_train.columns)\n",
    "\n",
    "    for x_feature in x_features:\n",
    "        if orders_of_integ[target] == orders_of_integ[x_feature] & orders_of_integ[target] != 0:\n",
    "            E_G = engle_granger(yx_train_original[target], yx_train_original[x_feature], trend = \"n\", method = \"t-stat\")\n",
    "            if E_G.pvalue < 0.05:\n",
    "                cointegr_dict[x_feature] = 1\n",
    "                ECM_X_const = sm.add_constant(yx_train_original[x_feature])\n",
    "                ECM_1 = OLS(yx_train_original[target], ECM_X_const, hasconst=True).fit(cov_type=(\"HC0\"))\n",
    "                ECM_res = ECM_1.resid\n",
    "                ECM_res = ECM_res.shift(1).dropna().reset_index(drop=True)\n",
    "                ECM_res = ECM_res.iloc[max_lag:].reset_index(drop=True)\n",
    "                ECM_residuals[x_feature] = ECM_res\n",
    "                ECM_OLSs[x_feature] = ECM_1\n",
    "\n",
    "            # ECM_1 = OLS(yx_train_original[target], yx_train_original[feature]).fit(cov_type=(\"HC0\"))\n",
    "            # ECM_res = ECM_1.resid\n",
    "            # ECM_adf = adfuller(ECM_res, autolag = \"t-stat\", regrression = \"nc\")[1]\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Step 2: Building a univariate model and finding the optimal l\n",
    "\n",
    "    BICs = []\n",
    "    for i in list(range(max_lag)):\n",
    "        model = AutoReg(y_train, lags=i).fit()\n",
    "        BICs.append(model.bic)\n",
    "\n",
    "    min_bic_ind = BICs.index(min(BICs))\n",
    "\n",
    "    # model = AutoReg(df_small.iloc[:,1], lags=min_bic_ind).fit()\n",
    "    # model.summary()\n",
    "\n",
    "    # Due to statsmodels weird properties, you can not test trained model on unseen y-data, but only on unseen X-data.\n",
    "    # Hence we need to perform some data manipulations to make the testing possible.\n",
    "\n",
    "    columns_y = []\n",
    "    for i in list(range(1, min_bic_ind+1)):\n",
    "        columns_y.append(target+\".L\"+str(i))\n",
    "\n",
    "    y_lags_df = pd.DataFrame(columns=columns_y)\n",
    "    for i in list(range(min_bic_ind)):\n",
    "        y_lags_df[columns_y[i]] = y_train.shift(i+1)\n",
    "\n",
    "    # Truncating lags of y at the maximum lag length\n",
    "    # y_lags_df.fillna(1, inplace=True)\n",
    "    y_lags_df = y_lags_df.iloc[max_lag:,:]\n",
    "    y_lags_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Step 2: Bulding augmented model and finding the optimal w for each Xi\n",
    "    \n",
    "    Xs = list(X_train.columns)\n",
    "\n",
    "    # Truncating y_train for model training at max_lag length\n",
    "    y_train_m = y_train.iloc[max_lag:]\n",
    "    y_train_m.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Defining dictionary to store all augmented models\n",
    "    aug_models = {}\n",
    "    feature_n_dfs = {}\n",
    "    feature_n_dfs_merge = [y_lags_df]\n",
    "    n_lags_for_xi = {}\n",
    "    \n",
    "    for x in Xs:\n",
    "        columns = []\n",
    "        for i in list(range(1, max_lag+1)):\n",
    "            columns.append(x + \".L\"+str(i))\n",
    "\n",
    "        feature_n_df = pd.DataFrame(columns=columns)\n",
    "        for i in list(range(max_lag)):\n",
    "            feature_n_df[columns[i]] = X_train[x].shift(i+1)\n",
    "\n",
    "        # NAs filled with are later automatically truncated by the AutoReg\n",
    "        feature_n_df.fillna(1, inplace=True)\n",
    "\n",
    "        feature_n_df = feature_n_df.iloc[max_lag:,:]\n",
    "        feature_n_df.reset_index(drop=True, inplace=True)\n",
    "        y_and_x_lags_df = pd.concat([y_lags_df, feature_n_df], axis=1)\n",
    "\n",
    "        BICs = []\n",
    "        #Why do I have max_lag-1 and then i+1?\n",
    "        # +1 is to not make X lags = 0\n",
    "        # y_and_x_lags_df_m = y_and_x_lags_df.iloc[:,:i+len(list(y_lags_df.columns))+1]\n",
    "        #y_and_x_lags_df.reset_index(drop=True, inplace=True)\n",
    "        for i in list(range(max_lag-1)):\n",
    "            model = AutoReg(y_train_m, lags=0, exog=y_and_x_lags_df.iloc[:,:i+len(list(y_lags_df.columns))+1]).fit()\n",
    "            BICs.append(model.bic)\n",
    "\n",
    "        min_bic_ind_aug = BICs.index(min(BICs))\n",
    "        #Full and Partial autocorrelation plot?\n",
    "        feature_n_df1 = y_and_x_lags_df\n",
    "        y_and_x_lags_df = y_and_x_lags_df.iloc[:,:min_bic_ind_aug+len(list(y_lags_df.columns))+1]\n",
    "        y_and_x_lags_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Adding ECM residual to the feature n dataset if it appears to be cointegrated with the target\n",
    "        if x in list(ECM_residuals.keys()):\n",
    "            ECM_res_name = x + \"_ECM_Res.L1\"\n",
    "            y_and_x_lags_df[ECM_res_name] = ECM_residuals[x]\n",
    "\n",
    "        model = AutoReg(y_train_m, lags=0, exog=y_and_x_lags_df).fit()\n",
    "\n",
    "        gr_test_df = pd.concat([X_train[x], y_train], axis=1)\n",
    "        granger_p_stat = grangercausalitytests(gr_test_df, maxlag=[min_bic_ind_aug+1])[min_bic_ind_aug+1][0]['params_ftest'][1]\n",
    "        if granger_p_stat >= 0.05:\n",
    "            aug_models[x] = model\n",
    "            feature_n_dfs[x] = feature_n_df1\n",
    "            feature_n_dfs_merge.append(y_and_x_lags_df.iloc[:,len(list(y_lags_df.columns)):])\n",
    "            n_lags_for_xi[x] = min_bic_ind_aug + 1\n",
    "            #model.summary()\n",
    "        elif granger_p_stat >= 0.01:\n",
    "            print(f'\\n\\nGranger causality from \"{target}\" to \"{x}\" can not be rejected with a p-value={granger_p_stat:.3}')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "        # aug_models[features[n]] = model\n",
    "        # feature_n_dfs[features[n]] = feature_n_df1\n",
    "        # feature_n_dfs_merge.append(feature_n_df)\n",
    "        # #model.summary()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        feature_n_dfs_merge = pd.concat(feature_n_dfs_merge, axis=1)\n",
    "\n",
    "        if len(feature_n_dfs_merge) == 0:\n",
    "                    print(\"\\n\\n\\n\\nZero lags of y have been selected and H0 of reverse causlity could not be rejected for any X.\\n\\n\\n\\n\")\n",
    "\n",
    "        fin_model = AutoReg(y_train_m, lags=0, exog=feature_n_dfs_merge).fit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Using backward elimination to drop insignificant features\n",
    "        # Defining critiacl p-value determining whether a feture is to be dropped\n",
    "        critical_p_value = 0.05\n",
    "        # Finding p-value of the lesat siginificant feature\n",
    "        max_p_value = max(fin_model.pvalues)\n",
    "        # Defining const_dropped to know whether we run Autoreg with or w/o const\n",
    "        const_dropped = False\n",
    "        while max_p_value >= critical_p_value:\n",
    "            # Column name of the least significant feature\n",
    "            least_sig_var = list(fin_model.params[np.where(fin_model.pvalues == max_p_value)[0]].index)[0]\n",
    "            # If least_sig_var is the constant we run Autoreg without it\n",
    "            if least_sig_var == \"const\":\n",
    "                fin_model = AutoReg(y_train_m, lags=0, exog=feature_n_dfs_merge, trend=\"n\").fit()\n",
    "                const_dropped = True\n",
    "\n",
    "            else:\n",
    "                # Dropping the least_sig_var from the df\n",
    "                feature_n_dfs_merge.pop(least_sig_var)\n",
    "                # If const has been dropped, we run Autoreg w/o it\n",
    "                if const_dropped:\n",
    "                    try:\n",
    "                        fin_model = AutoReg(y_train_m, lags=0, exog=feature_n_dfs_merge, trend=\"n\").fit()\n",
    "                    except ValueError:\n",
    "                        print(\"\\n\\n\\n\\nNo coefficients appear to be significant in the estimated model.\\n\\n\\n\\n\")\n",
    "                else:\n",
    "                    fin_model = AutoReg(y_train_m, lags=0, exog=feature_n_dfs_merge).fit()\n",
    "\n",
    "            # At the end of each iteration we find the new highest p-value        \n",
    "            max_p_value = max(fin_model.pvalues)\n",
    "\n",
    "        # Defining list of all significant variables (except for const - because it is not in the data)\n",
    "        names_of_sig_vars = [n for n in list(fin_model.params.index) if n!= \"const\"]\n",
    "\n",
    "\n",
    "        y_pred_in = fin_model.predict()\n",
    "        MAE_train = np.nanmean(abs(y_pred_in - y_train_m))\n",
    "\n",
    "        if orders_of_integ[target] > 0:\n",
    "\n",
    "            # Calculating train scores in the original scale\n",
    "\n",
    "            if stationarity_method == 0:\n",
    "                y_train_m_destat = y_train_m.copy()\n",
    "                y_train_m_destat.loc[-1] = y_original.iloc[max_lag]\n",
    "                y_train_m_destat.index = y_train_m_destat.index + 1\n",
    "                y_train_m_destat = y_train_m_destat.sort_index()\n",
    "                y_train_m_destat = y_train_m_destat.cumsum()\n",
    "\n",
    "\n",
    "                y_pred_in_destat = y_pred_in.copy()\n",
    "                y_pred_in_destat.loc[-1] = y_original.iloc[max_lag]\n",
    "                y_pred_in_destat.index = y_pred_in_destat.index + 1\n",
    "                y_pred_in_destat = y_pred_in_destat.sort_index()\n",
    "                y_pred_in_destat = y_pred_in_destat.cumsum()\n",
    "\n",
    "                MAE_train_destat = np.nanmean(abs(y_pred_in_destat - y_train_m_destat))\n",
    "\n",
    "            elif stationarity_method == 1:\n",
    "                y_train_m_destat = y_train_m.copy()\n",
    "                y_train_m_destat.loc[-1] = y_logged.iloc[max_lag]\n",
    "                y_train_m_destat.index = y_train_m_destat.index + 1\n",
    "                y_train_m_destat = y_train_m_destat.sort_index()\n",
    "                y_train_m_destat = np.exp(y_train_m_destat.cumsum())\n",
    "\n",
    "\n",
    "                y_pred_in_destat = y_pred_in.copy()\n",
    "                y_pred_in_destat.loc[-1] = y_logged.iloc[max_lag]\n",
    "                y_pred_in_destat.index = y_pred_in_destat.index + 1\n",
    "                y_pred_in_destat = y_pred_in_destat.sort_index()\n",
    "                y_pred_in_destat = np.exp(y_pred_in_destat.cumsum())\n",
    "\n",
    "                MAE_train_destat = np.nanmean(abs(y_pred_in_destat - y_train_m_destat))\n",
    "\n",
    "        # Coppying data for ECM imlplementation\n",
    "        y_test_non_stat = y_test.copy()\n",
    "        X_test_non_stat = X_test.copy()\n",
    "\n",
    "        # Stationarising test data\n",
    "        stationarity_df_test = pd.concat([y_test, X_test], axis=1)\n",
    "        features = list(stationarity_df_test.columns)\n",
    "\n",
    "        if stationarity_method == 0:\n",
    "        # if transformation is simple differencing\n",
    "            for feature in features:\n",
    "                # Continue if the feature was found to be stationary withoud tranformation\n",
    "                if orders_of_integ[feature] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    order = orders_of_integ[feature]\n",
    "                    integr_list = list(range(order, order+1))\n",
    "                    # Difference o times as with the train data\n",
    "                    for o in integr_list:\n",
    "                        stationarity_df_test[feature] = stationarity_df_test[feature].diff()\n",
    "\n",
    "        elif stationarity_method == 1:\n",
    "        # if transformation is log differencing\n",
    "            for feature in features:\n",
    "                # Continue if the feature was found to be stationary withoud tranformation\n",
    "                if orders_of_integ[feature] == 0:\n",
    "                        continue\n",
    "                else:\n",
    "                    # Check whether any constants were added to the training data\n",
    "                    if const_counters[feature] > 0:\n",
    "                        stationarity_df_test[feature] = stationarity_df_test[feature] + const_counters[feature]\n",
    "                    # Logging the data\n",
    "                    stationarity_df_test[feature] = np.log(stationarity_df_test[feature])\n",
    "                    order = orders_of_integ[feature]\n",
    "                    integr_list = list(range(order, order+1))\n",
    "                    # Difference o times as with the train data\n",
    "                    for o in integr_list:\n",
    "                        stationarity_df_test[feature] = stationarity_df_test[feature].diff()\n",
    "\n",
    "\n",
    "        stationarity_df_test.dropna(inplace=True)\n",
    "        stationarity_df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        #Finding the maximum seleceted lag length to truncate the test data appropriately\n",
    "        selected_lag_lens = []\n",
    "        selected_lag_lens.append(min_bic_ind)\n",
    "        for x_name, lag_len in n_lags_for_xi.items():\n",
    "            selected_lag_lens.append(lag_len)\n",
    "\n",
    "        max_sel_lag = max(selected_lag_lens)\n",
    "\n",
    "        # Formatting y\n",
    "        y_lags_df = pd.DataFrame(columns=columns_y)\n",
    "        for i in list(range(min_bic_ind)):\n",
    "            y_lags_df[columns_y[i]] = y_test.shift(i+1)\n",
    "\n",
    "        # Truncating lags of y at the maximum lag length\n",
    "        # y_lags_df.fillna(1, inplace=True)\n",
    "        y_lags_df = y_lags_df.iloc[max_sel_lag:,:]\n",
    "        y_lags_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_data.append(y_lags_df)\n",
    "\n",
    "\n",
    "        #Formatting Xs and implementing ECM on the test data\n",
    "        Cointegrated_Xs = list(ECM_residuals.keys())\n",
    "\n",
    "        for x_name, lag_len in n_lags_for_xi.items():\n",
    "            columns = []\n",
    "            for i in list(range(1, lag_len+1)):\n",
    "                columns.append(x_name+\".L\"+str(i))\n",
    "\n",
    "            feature_x_df = pd.DataFrame(columns=columns)\n",
    "            for i in list(range(lag_len)):\n",
    "                feature_x_df[columns[i]] = X_test[x_name].shift(i+1)\n",
    "            \n",
    "            feature_x_df = feature_x_df.iloc[max_sel_lag:,:]\n",
    "            feature_x_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            if x_name in Cointegrated_Xs:\n",
    "                ECM_OLS = ECM_OLSs[x_name]\n",
    "                X_test_ECM = sm.add_constant(X_test_non_stat[x_name])\n",
    "                y_ECM_pred = ECM_OLS.predict(X_test_ECM)\n",
    "                y_test_ECM_res = y_ECM_pred - y_test_non_stat\n",
    "                y_test_ECM_res = y_test_ECM_res.shift(1)\n",
    "                y_test_ECM_res = y_test_ECM_res.iloc[max_sel_lag:]\n",
    "                y_test_ECM_res.reset_index(drop=True, inplace=True)\n",
    "                feature_x_df[x_name + \"_ECM_Res.L1\"] = y_test_ECM_res\n",
    "            \n",
    "            test_data.append(feature_x_df)\n",
    "    \n",
    "        # Merging y and Xs\n",
    "        test_data = pd.concat(test_data, axis=1)\n",
    "        # Only keeping the significant features\n",
    "        test_data = test_data[names_of_sig_vars]\n",
    "        # Truncating y_test, so its length corresponds to that of y_train_m\n",
    "        y_test = y_test.iloc[max_sel_lag:]\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        first_oos_ind = len(y_train_m)\n",
    "        last_oos_ind = first_oos_ind + len(y_test) - 1\n",
    "        y_pred_out = fin_model.predict(start=first_oos_ind, end=last_oos_ind, exog_oos=test_data)\n",
    "        y_pred_out.reset_index(drop=True, inplace=True)\n",
    "        MAE_test = np.nanmean(abs(y_pred_out - y_test))\n",
    "\n",
    "        if orders_of_integ[target] > 0:\n",
    "\n",
    "            # Calculating test scores in the original scale\n",
    "            if stationarity_method == 0:\n",
    "                y_pred_out_destat = y_pred_out.copy()\n",
    "                y_pred_out_destat.loc[-1] = y_test_non_stat.iloc[max_sel_lag]\n",
    "                y_pred_out_destat.index = y_pred_out_destat.index + 1\n",
    "                y_pred_out_destat = y_pred_out_destat.sort_index()\n",
    "                y_pred_out_destat = y_pred_out_destat.cumsum()\n",
    "\n",
    "                y_test_non_stat_destat = y_test_non_stat.copy()\n",
    "                y_test_non_stat_destat = y_test_non_stat_destat.iloc[max_sel_lag:]\n",
    "                y_test_non_stat_destat.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "                MAE_test_destat = np.nanmean(abs(y_pred_out_destat - y_test_non_stat_destat))\n",
    "\n",
    "            elif stationarity_method == 1:\n",
    "                y_test_logged = np.log(y_test_non_stat)\n",
    "                y_pred_out_destat = y_pred_out.copy()\n",
    "                y_pred_out_destat.loc[-1] = y_test_logged.iloc[max_sel_lag]\n",
    "                y_pred_out_destat.index = y_pred_out_destat.index + 1\n",
    "                y_pred_out_destat = y_pred_out_destat.sort_index()\n",
    "                y_pred_out_destat = np.exp(y_pred_out_destat.cumsum())\n",
    "\n",
    "                y_test_non_stat_destat = y_test_logged.copy()\n",
    "                y_test_non_stat_destat = y_test_non_stat_destat.iloc[max_sel_lag:]\n",
    "                y_test_non_stat_destat.reset_index(drop=True, inplace=True)\n",
    "                y_test_non_stat_destat = np.exp(y_test_non_stat_destat)\n",
    "\n",
    "                MAE_test_destat = np.nanmean(abs(y_pred_out_destat - y_test_non_stat_destat))\n",
    "\n",
    "            MAE = {\"train\": MAE_train_destat, \"test\": MAE_test_destat}\n",
    "        else:\n",
    "            MAE = {\"train\": MAE_train, \"test\": MAE_test}\n",
    "            y_train_m_destat = y_train_m\n",
    "            y_test_non_stat_destat = y_test\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        MAE_nonstat = {\"train\": MAE_train, \"test\": MAE_test}\n",
    "        logging.info(\"Check\")\n",
    "        destat_data = {\"y_train\": y_train_m_destat, \"y_test\": y_test_non_stat_destat,\n",
    "                        \"stationarity_method\": stationarity_method,\n",
    "                        \"y_integ_order\": orders_of_integ[target]}\n",
    "\n",
    "        Model_Data = Sun_Model(fin_model, fin_model.summary(), aug_models, MAE,\n",
    "                                y_train_m, feature_n_dfs_merge,\n",
    "                                y_test, test_data,\n",
    "                                y_pred_out, destat_data)\n",
    "        #return fin_model, aug_models, feature_n_dfs, feature_n_dfs_merge, MAE, Sun_Model1\n",
    "        return Model_Data\n",
    "    except ValueError:\n",
    "        logging.error(\"Can not reject that the target variable 'reverse causes' independent features.\")\n",
    "\n",
    "\n",
    "#Reading in the data\n",
    "df_aapl = pd.read_csv(\"df_aaple.csv\")\n",
    "# Truncating the dataw\n",
    "aapl_medium = df_aapl.iloc[:2000,:16]\n",
    "aapl_long = df_aapl.iloc[:,:16]\n",
    "\n",
    "df_jpm = pd.read_csv(\"jpm.csv\")\n",
    "jpm_medium = df_jpm.iloc[:2000,:16]\n",
    "jpm_long = df_jpm.iloc[:,:16]\n",
    "\n",
    "df_fb = pd.read_csv(\"fb.csv\")\n",
    "fb_medium = df_fb.iloc[:2000,:16]\n",
    "fb_long = df_fb.iloc[:,:16]\n",
    "\n",
    "df_google = pd.read_csv(\"googl.csv\")\n",
    "google_medium = df_google.iloc[:2000,:16]\n",
    "google_long = df_google.iloc[:,:16]\n",
    "\n",
    "\n",
    "df_dehli = pd.read_csv(\"dehli_weather.csv\")\n",
    "\n",
    "df_air_q = pd.read_csv(\"AirQualityUCI.csv\")\n",
    "# df_air_q.drop(\"Date\", axis=1, inplace=True) CO(GT)\n",
    "# pd.DataFrame.to_csv(df_air_q, \"df_air_q_no_date.csv\", index=True)\n",
    "\n",
    "\n",
    "airpassengers_df = pmd.datasets.load_airpassengers(as_series = True)\n",
    "msft_pmd_df = pmd.datasets.load_msft()\n",
    "msft_pmd_df = msft_pmd_df.iloc[:,:-1]\n",
    "\n",
    "\n",
    "#fin_model, aug_models, dfs, dfs_merged, MAE, Model = algo(df=df_medium, target=\"Close\", max_lag=20)\n",
    "\n",
    "Model_Data = algo(df=aapl_long, target=\"Close\", max_lag=20, stationarity_method = 0, test_size=0.2)\n",
    "\n",
    "apple_stat = pd.concat([Model_Data.train_y, Model_Data.train_x], axis=1)\n",
    "apple_stat.to_csv(\"aaple_stat.csv\", index=True)\n",
    "\n",
    "\n",
    "print(Model_Data.summary)\n",
    "\n",
    "\n",
    "print(Model_Data.MAE)\n",
    "# print(Model_Data.train_y)\n",
    "\n",
    "\n",
    "filename = 'Sun_Model_Data'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(Model_Data,outfile)\n",
    "outfile.close()\n",
    "\n",
    "# apple_long_evoML = pd.concat([Model_Data.y_train])\n",
    "\n",
    "# {'train': 1.2125139241871459, 'test': 1.199242993289765}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecda2226e1d4421bab328a480e856f7e2efe4f4ade8deb7f822fd3214f8320da"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('test1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
